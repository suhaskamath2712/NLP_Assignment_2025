{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OerOiObFAes2"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**DS: 207 Introduction to Natural Language Processing**\n",
        "\n",
        "**Due (Feb 6, 16:59 PM)**\n",
        "\n",
        "Development & Design: Debarpan Bhattacharya & Nicy Scaria.\n",
        "\n",
        "Testing: Kinshuk Vasisht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8KnPBvJEzX9"
      },
      "source": [
        "The goal of this assignment is introduce the basics of text processing, by building a few text classifiers, and learning to represent words.\n",
        "\n",
        "You'll have to add your code wherever you see the comment `# ADD YOUR CODE HERE`. Please make a copy of this assignment, and you can use Google Colab notebooks to work on this. Later, you can download this notebook as a python file and submit it as per the following instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qpIrZWGbSK0"
      },
      "source": [
        "## Submission Instructions:\n",
        "\n",
        "1. In the notebook, a few example codes are provided, whereas a few are kept blank for you to fill them up.\n",
        "2. The evaluation will be based on the results obtained by functions impleted by you. Do not change the print statements having `EVALUATION` as they are used for auto-evaluation.\n",
        "3. **Submission file(s)**:\n",
        "  \n",
        "  * Save this `.ipynb` to your drive, complete the required code blocks and run the notebook.\n",
        "  * After completing this assignment, download the notebook as`.py`. Name it as `SAPname_SRno_assignment1.py`, where `SAPname` refers to your name as per SAP record, and `SRno` refers to the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would name it as `Twyla_Linda_15329_assignment1.py`.\n",
        "  *   The files associated with the Word2Vec training, i.e., the `model.pt`, `vocab.pt`, `word_embeddings.npy` and `loss.json`, will be downloaded and saved in a folder with the name `SAPname_SRno`. Zip this folder along with the `.py` file, save it as `SAPname_SRno_assigment1` and upload on MS Teams. The zip folder should contain: (1) `SAPname_SRno_assignment1.py`, (2) a subfolder called `SAPname_SRno` which will contain `model.pt`, `vocab.pt` , `word_embeddings.npy` and `loss.json`.\n",
        "\n",
        "\n",
        "Because submissions are auto-graded, please ensure that the naming of the files is consistent with the instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAVFm89QjHP6"
      },
      "source": [
        "## Part I Text Classification (TA: Debarpan Bhattacharya)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQQTcvWL7Qtm"
      },
      "source": [
        "**Dataset**:\n",
        "\n",
        "We will dive into a basic text-based sentiment classification task. The dataset consists of sentences with two different kinds of sentiments- `positive`, and `negative` sentiments. Following are a set of examples,\n",
        "\n",
        "* **`positive`**: *I really like your new haircut!*\n",
        "* **`negative`**: *Your new haircut is awful!*\n",
        "\n",
        "The Dataset has a training set (`train_data.csv`- provided), a validation set (`val_data.csv`- provided) and a blind test set (`test_data.csv`- not provided). The notebook uses a `test_data.csv` file, but it is just a duplicate of `val_data.csv`, and the blind `test_data.csv` will replace it while grading your solutions.\n",
        "\n",
        "**Important**: Fix seed as 42 whenever performing any randomized operations, e.g., initializing ML models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_gUoUztJVwX"
      },
      "source": [
        "### Download the dataset required for the assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waznd1GRGwFx",
        "outputId": "6fbd727f-2e17-4675-bc56-af978fd40a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-01-16 10:22:59--  https://docs.google.com/spreadsheets/d/176-KrOP8nhLpoW91UnrOY9oq_-I0XYNKS1zmqIErFsA/gviz/tq?tqx=out:csv&sheet=train_data.csv\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.170.102, 142.251.170.139, 142.251.170.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.170.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘train_data.csv’\n",
            "\n",
            "train_data.csv          [          <=>       ]  15.12M  7.42MB/s    in 2.0s    \n",
            "\n",
            "2026-01-16 10:23:04 (7.42 MB/s) - ‘train_data.csv’ saved [15851033]\n",
            "\n",
            "--2026-01-16 10:23:04--  https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.170.102, 142.251.170.139, 142.251.170.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.170.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘val_data.csv’\n",
            "\n",
            "val_data.csv            [     <=>            ]   5.14M  5.40MB/s    in 1.0s    \n",
            "\n",
            "2026-01-16 10:23:06 (5.40 MB/s) - ‘val_data.csv’ saved [5387545]\n",
            "\n",
            "--2026-01-16 10:23:06--  https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.170.102, 142.251.170.139, 142.251.170.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.170.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘test_data.csv’\n",
            "\n",
            "test_data.csv           [     <=>            ]   5.14M  5.44MB/s    in 0.9s    \n",
            "\n",
            "2026-01-16 10:23:07 (5.44 MB/s) - ‘test_data.csv’ saved [5387545]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download train data\n",
        "!wget -O train_data.csv \"https://docs.google.com/spreadsheets/d/176-KrOP8nhLpoW91UnrOY9oq_-I0XYNKS1zmqIErFsA/gviz/tq?tqx=out:csv&sheet=train_data.csv\"\n",
        "\n",
        "# download validation data\n",
        "!wget -O val_data.csv \"https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\"\n",
        "\n",
        "# download test data\n",
        "!wget -O test_data.csv \"https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d7B1k-8XOEqo",
        "outputId": "4e463873-44c9-4d57-f12a-9e155bda5d3f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9889e244-e3f6-4ab6-892e-2ea156e2e751\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've watched this documentary twice - and alth...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is probably the worst movie I've seen in ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Superb story of a dedicated young teacher who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;br /&gt;&lt;br /&gt;Spoilers&lt;br /&gt;&lt;br /&gt;I'm going to b...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What can be said, really... \"The Tenant\" is a ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9889e244-e3f6-4ab6-892e-2ea156e2e751')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9889e244-e3f6-4ab6-892e-2ea156e2e751 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9889e244-e3f6-4ab6-892e-2ea156e2e751');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  I've watched this documentary twice - and alth...  positive\n",
              "1  This is probably the worst movie I've seen in ...  negative\n",
              "2  Superb story of a dedicated young teacher who ...  positive\n",
              "3  <br /><br />Spoilers<br /><br />I'm going to b...  negative\n",
              "4  What can be said, really... \"The Tenant\" is a ...  positive"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Read data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_data.csv')\n",
        "df_val = pd.read_csv('val_data.csv')\n",
        "\n",
        "# Note that we will change the test file\n",
        "# when we grade assignments  ...\n",
        "# For now it is the same as the validation set\n",
        "df_test = pd.read_csv('test_data.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kxi5DtdKOt7e"
      },
      "outputs": [],
      "source": [
        "#@title Prepare training, validation and test data.\n",
        "\n",
        "X_train, y_train = df.review.values.tolist(), df.sentiment.values.tolist()\n",
        "X_val, y_val = df_val.review.values.tolist(), df_val.sentiment.values.tolist()\n",
        "X_test, y_test = df_test.review.values.tolist(), df_test.sentiment.values.tolist()\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "\n",
        "# converting the sentiment labels into labels\n",
        "# class 0 for negagtive, and class 1 for positive\n",
        "y_train = [labels.index(i) for i in y_train]\n",
        "y_val = [labels.index(i) for i in y_val]\n",
        "y_test = [labels.index(i) for i in y_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGiZtZSpQmk5"
      },
      "source": [
        "#### Approach 1: Rule based classification\n",
        "\n",
        "The rule-based classification works using a few hand-crafted rules. In sentiment classification, few words are associated with positive sentiment and few others with negative sentiment. Let's attempt to build a classifier that predicts the sentiment of the reviews based on such words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRt7jQNVaPO2"
      },
      "source": [
        "Let's first write a few rules to extract important features about the input movie review. As an example, we provide a function that counts the number of `good` and `bad` words in the input.\n",
        "\n",
        "Below, you can see a sample rule-based classification system. Later, you will be building one yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JT_Et4xpPThR"
      },
      "outputs": [],
      "source": [
        "def sample_extract_features(X):\n",
        "    \"\"\"\n",
        "    Extracts features from a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: features extracted from X.\n",
        "    \"\"\"\n",
        "\n",
        "    features = {}\n",
        "    X_split = X.split(' ')\n",
        "\n",
        "    # Count the number of \"good words\" and \"bad words\" in the text\n",
        "    good_words = ['love', 'good', 'brilliant', 'fantastic', 'amazing', 'great']\n",
        "    bad_words = ['hate', 'bad', 'horrible', 'awful', 'terrible', 'mess', 'frustating', 'frustatingly']\n",
        "\n",
        "    features['good_word_count'], features['bad_word_count'] = 0, 0\n",
        "    for x in X_split:\n",
        "        if x in good_words:\n",
        "            features['good_word_count'] = features.get('good_word_count', 0) + 1\n",
        "        if x in bad_words:\n",
        "            features['bad_word_count'] = features.get('bad_word_count', 0) + 1\n",
        "\n",
        "    # The \"bias\" value can be set to one, to allow us to assign a \"default\" score to the text\n",
        "    features['bias'] = 1\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PQea1rsPCCVz"
      },
      "outputs": [],
      "source": [
        "def sample_predict(X, feature_weights):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "        feature_weights: weightage of different features.\n",
        "\n",
        "    Returns:\n",
        "        int: binary sentiment represented by 0/1.\n",
        "    \"\"\"\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    # Here we just multiply the feature value\n",
        "    # with its corresponding weight and aggregate\n",
        "    for feat_name, feat_value in sample_extract_features(X).items():\n",
        "        score = score + feat_value * feature_weights[feat_name]\n",
        "\n",
        "    # the prediction is based on whether the aggregated score is above 0 or not\n",
        "    if score > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mld2X8uP_1DK"
      },
      "outputs": [],
      "source": [
        "def get_sample_features_weights():\n",
        "  \"\"\"\n",
        "    To obtain feature weightage for different features.\n",
        "\n",
        "    Args:\n",
        "        None here.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: feature names and their weightage.\n",
        "    \"\"\"\n",
        "  # Based on the selected features, you can manually assign them weights\n",
        "  feature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}\n",
        "  return feature_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5soNJvv0AZeM"
      },
      "outputs": [],
      "source": [
        "#@title Computing the accuracy of the system\n",
        "\n",
        "def calculate_accuracy(Y_true, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculates accuracy of predictions given the ground truth.\n",
        "\n",
        "    Args:\n",
        "        Y_true (list): Ground truth labels.\n",
        "        Y_pred (list): Predictions.\n",
        "\n",
        "    Returns:\n",
        "        float: Prediction accuracy in range [0.0-100.0].\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = len(Y_true)\n",
        "\n",
        "    # verify if we have the same number of predictions as labels\n",
        "    assert len(Y_true) == len(Y_pred)\n",
        "\n",
        "    # count the number of correct predictions\n",
        "    for y_true, y_pred in zip(Y_true, Y_pred):\n",
        "        if y_true == y_pred:\n",
        "            correct += 1.0\n",
        "\n",
        "    if total > 0:\n",
        "        return 100. * correct / total\n",
        "\n",
        "    # return 0 if there the total number of examples are zero\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUJxIXK6COeY",
        "outputId": "babbb095-2e2f-4045-9496-e0125bb95486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60.5\n"
          ]
        }
      ],
      "source": [
        "#@title Putting the sample rule-based classifier together\n",
        "\n",
        "# get the sample weights\n",
        "sample_feature_weights = get_sample_features_weights()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# for each test example, make a prediction\n",
        "for input_example in X_test:\n",
        "    y = sample_predict(input_example, sample_feature_weights)\n",
        "    predictions.append(y)\n",
        "\n",
        "# compute and print the accuracy\n",
        "print (calculate_accuracy(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5zIgT9sGejB"
      },
      "source": [
        "As you can observe here, the model achieves about 60% accuracy (note that the performance of a random classifier would be close to 50%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUNaPV2KGyB1"
      },
      "source": [
        "### Build your own rule-based classifier (10 marks)\n",
        "\n",
        "Your have to write your own `extract_features`, `get_feature_weights` and `predict` functions for your rule-based classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Aio_eGugGuuZ"
      },
      "outputs": [],
      "source": [
        "def isAllCaps(word):\n",
        "    return word.isupper() and len(word) > 1\n",
        "\n",
        "def extract_features(X):\n",
        "    \"\"\"\n",
        "    Extracts features from a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: features extracted from X.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    features['bias'] = 1\n",
        "\n",
        "    import re\n",
        "\n",
        "    # --- lexicons (tweak freely) ---\n",
        "    pos_words = {\n",
        "        \"love\",\"loved\",\"like\",\"liked\",\"great\",\"amazing\",\"awesome\",\"fantastic\",\"brilliant\",\"excellent\",\n",
        "        \"wonderful\",\"best\",\"perfect\",\"enjoy\",\"enjoyed\",\"nice\",\"good\",\"beautiful\",\"happy\",\"superb\"\n",
        "    }\n",
        "    neg_words = {\n",
        "        \"hate\",\"hated\",\"bad\",\"awful\",\"terrible\",\"horrible\",\"worst\",\"boring\",\"waste\",\"poor\",\n",
        "        \"disappointing\",\"disappointed\",\"annoying\",\"mess\",\"stupid\",\"dull\",\"ugly\",\"sad\",\"ridiculous\",\"painful\"\n",
        "    }\n",
        "    negators = {\"not\",\"no\",\"never\",\"n't\",\"hardly\",\"rarely\"}\n",
        "    intensifiers = {\"very\",\"really\",\"so\",\"extremely\",\"quite\",\"too\",\"super\"}\n",
        "\n",
        "    # tokenize: lowercase words + keep contractions like \"don't\"\n",
        "    tokens = re.findall(r\"[a-z]+'t|[a-z]+\", X.lower())\n",
        "\n",
        "    # --- initialize ALL feature keys (important for LR block later) ---\n",
        "    features[\"pos_word_count\"] = 0\n",
        "    features[\"neg_word_count\"] = 0\n",
        "\n",
        "    # count words + handle simple negation scope (previous token)\n",
        "    prev = \"\"\n",
        "    for t in tokens:\n",
        "        multiplier = 1\n",
        "\n",
        "        if prev in intensifiers:\n",
        "            multiplier += 1\n",
        "        if isAllCaps(prev):\n",
        "            multiplier += 1\n",
        "        if isAllCaps(t):\n",
        "            multiplier += 1\n",
        "\n",
        "        # Only count words that are in our sentiment lexicons.\n",
        "        if t in pos_words:\n",
        "            # flip if negation precedes a positive word: \"not good\" -> negative\n",
        "            if prev in negators:\n",
        "                features[\"neg_word_count\"] += multiplier\n",
        "            else:\n",
        "                features[\"pos_word_count\"] += multiplier\n",
        "        elif t in neg_words:\n",
        "            # flip if negation precedes a negative word: \"not terrible\" -> positive\n",
        "            if prev in negators:\n",
        "                features[\"pos_word_count\"] += multiplier\n",
        "            else:\n",
        "                features[\"neg_word_count\"] += multiplier\n",
        "\n",
        "        prev = t\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SY0ioIklQp-J"
      },
      "outputs": [],
      "source": [
        "def get_feature_weights():\n",
        "    return {'pos_word_count': 1.0, 'neg_word_count': -1.0, 'bias': 0.5}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hKqwGoZ-H7x4"
      },
      "outputs": [],
      "source": [
        "def predict(X, feature_weights):\n",
        "    feats = extract_features(X)\n",
        "    score = 0.0\n",
        "\n",
        "    for k, v in feats.items():\n",
        "        score += v * feature_weights.get(k, 0.0)\n",
        "\n",
        "    return 1 if score > 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxv4-0mJIJS5",
        "outputId": "cef11d49-ae21-430b-d802-749101c1844e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION of rule-based classifier is: 66.8\n"
          ]
        }
      ],
      "source": [
        "#@title Evaluating your rule-based classifier\n",
        "\n",
        "## Please do not change anything in this code block.\n",
        "\n",
        "feature_weights = get_feature_weights()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = predict(input_example, feature_weights)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (f\"EVALUATION of rule-based classifier is: {calculate_accuracy(y_test, predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u1EmkV5IaDR"
      },
      "source": [
        "You will be evaluated based on the performance of your rule-based classifier. Please note that the sample rule-based classifier achieves about 60% accuracy. Anything below 60% will not yield any points.\n",
        "\n",
        "***Please do not change the evaluation code in the block above, as it would be used to grade your classifier***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZqIJHzbJiQe"
      },
      "source": [
        "Below, we will directly learn the feature weights, rather than manually assigning those ourselves as manual assignment can be error-prone and labor intensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6FTuS2MOgDUU"
      },
      "outputs": [],
      "source": [
        "#@title Learning the weights of extracted features using logistic regression.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def get_sample_learnable_weights(X_data, Y_data, sample_extract_features):\n",
        "    \"\"\"\n",
        "    Learn feature weights using the training data.\n",
        "\n",
        "    Args:\n",
        "        X_data (list of strings): All the text data points in training data.\n",
        "        Y_data (list of int): Ground truth labels for text data points in X_data.\n",
        "        sample_extract_features: A Function that extracts features from text sample.\n",
        "                                 The sample_extract_features function should be of the\n",
        "                                 same format as sample_extract_features(X) function\n",
        "                                 implemented above.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: feature names and their learned weights.\n",
        "    \"\"\"\n",
        "    # training a logistic regression model for classification using\n",
        "    # the features obtained by sample_extract_features(X) function.\n",
        "\n",
        "    # get all feature names\n",
        "    feature_names = list(sample_extract_features(X_data[0]).keys())\n",
        "\n",
        "    # Below code snippet accumulates features extracted from all the\n",
        "    # text data points in X_data.\n",
        "    all_features = []\n",
        "    # iterate over all text data points in X_data.\n",
        "    for input_example in X_data:\n",
        "      feature = [] # to store features extracted from input_example.\n",
        "      feat_dict = sample_extract_features(input_example)\n",
        "      # iterate over different feature names and store the corresonding values.\n",
        "      for name in feature_names:\n",
        "        feature.append(feat_dict[name])\n",
        "      all_features.append(feature) # append features obtained from input_example to all_features.\n",
        "\n",
        "    # Below, we show how to fit a logistic regression (LR) model using the features and the target labels.\n",
        "    # We use Sklearn's 'LogisticRegression' to do this. While initiating, 'fit_intercept' is set False,\n",
        "    # because 'bias' is already included in the extracted feature (see sample_extract_features() implementation).\n",
        "    # Also, random state is set to 42 to avoid different initialization of the model while running the codebook\n",
        "    # multiple times.\n",
        "    clf = LogisticRegression(fit_intercept=False, random_state=42).fit(all_features, Y_data)\n",
        "\n",
        "    # As training a logistic regression model assigns weights to each of the features (refer class notes),\n",
        "    # these weights are learnable and we will use them as learned feature weights.\n",
        "\n",
        "    # extract feature weights.\n",
        "    coffs = clf.coef_[0]\n",
        "\n",
        "    # convert to dictionary\n",
        "    coffs_dict = {feature_names[i]: coffs[i] for i in range(len(feature_names))}\n",
        "\n",
        "    return coffs_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsCdOCq6mtJ1",
        "outputId": "7dcf0f23-6998-40a8-beee-68c25853081b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63.775\n"
          ]
        }
      ],
      "source": [
        "#@title Putting the sample rule-based classifier with learnable weights together\n",
        "\n",
        "# get the sample weights\n",
        "sample_feature_weights_lr = get_sample_learnable_weights(X_train, y_train, sample_extract_features)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = sample_predict(input_example, sample_feature_weights_lr)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (calculate_accuracy(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K29X_7C4zfVH"
      },
      "source": [
        "As you can observe here, the model achieves about 63% accuracy (note that the performance of a random classifier would be close to 50%). Note that, the performance significantly improves by making the weights learnable as compared to the manual weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ztqIifzwud"
      },
      "source": [
        "## How does learnable weights-based classifier work on the features you implemented (3 marks)\n",
        "\n",
        "Your don't have to write anything here except simply running the below kernel.\n",
        "Depending upon the quality of your features, your classifier performance will vary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvVJYi980W0y",
        "outputId": "47520982-8a37-4aa3-aaea-9e5a8bda25b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION of rule-based classifier with learnable weights is: 76.6\n"
          ]
        }
      ],
      "source": [
        "#@title Evaluating your rule-based classifier with learnable weights.\n",
        "\n",
        "## Please do not change anything in this code block.\n",
        "feature_weights_lr = get_sample_learnable_weights(X_train, y_train, extract_features)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = predict(input_example, feature_weights_lr)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (f\"EVALUATION of rule-based classifier with learnable weights is: {calculate_accuracy(y_test, predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "E2jMSsYLwY3L",
        "outputId": "5d624f96-33a8-4e02-e644-0ed090891038"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Visualization and comparison between manual and learnable weights.\n",
        "\n",
        "# visualize and compare the manual weights vs learnt weights.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "feature_names = list(sample_feature_weights.keys())\n",
        "manual_weights = [sample_feature_weights[name] for name in feature_names]\n",
        "learnt_weights = [sample_feature_weights_lr[name] for name in feature_names]\n",
        "weights_dict = {\n",
        "    'Manual weights': manual_weights,\n",
        "    'Learnt weights': learnt_weights,\n",
        "}\n",
        "\n",
        "x = np.arange(len(feature_names))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in weights_dict.items():\n",
        "    offset = width * multiplier\n",
        "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=3)\n",
        "    multiplier += 1\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('weight')\n",
        "ax.set_title('Manual vs learnable weights comparison')\n",
        "ax.set_xticks(x + width, feature_names)\n",
        "ax.legend(loc='upper left', ncols=2)\n",
        "ax.set_ylim(-2, 2)\n",
        "\n",
        "plt.savefig('manual_vs_learned_weights.png')\n",
        "# Refer to the saved manual_vs_learned_weights.png to see how manual and learned weights compare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TirWZWbRjnJH"
      },
      "source": [
        "### Approach 2: Bag-of-Words (BoW) (10 marks)\n",
        "\n",
        "The BoW vector representations is based on the unordered counts of words piece of text (similar to a \"bag\" of words).\n",
        "\n",
        "Let's attempt to build a classifier that tries to classify the reviews based on such vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TeavAClasyz6"
      },
      "outputs": [],
      "source": [
        "# implement your BoW classifier. In case the total number of words are very large,\n",
        "# consider using top-k most frequent words (e.g. k=10,000) while creating BoW vectors.\n",
        "\n",
        "# The below function finds out all words present in the corpus and assigns each word to\n",
        "# an index.\n",
        "\n",
        "def word_to_idx_BOW(X_data, K=10000):\n",
        "    \"\"\"\n",
        "    Generates a set of word-index pairs after analyzing all words present in entire\n",
        "    data corpus X_data.\n",
        "\n",
        "    Args:\n",
        "        X_data (list of strings): All the text data points in training data.\n",
        "        K (int): The number of most frequent words to be considered.\n",
        "                 In case (K-p)th to (K+q)th words in most frequent words order\n",
        "                 have same frequency, you can choose p words randomly from those\n",
        "                 (p+q) number of words.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: words as keys and indices as values.\n",
        "    \"\"\"\n",
        "    word_to_idx = {}\n",
        "    # ADD YOUR CODE HERE\n",
        "    for i in range(len(X_data)):\n",
        "        words = X_data[i].split(' ')\n",
        "        for word in words:\n",
        "            if word not in word_to_idx:\n",
        "                word_to_idx[word] = 0\n",
        "            word_to_idx[word] += 1\n",
        "\n",
        "        if len(word_to_idx) > K:\n",
        "            break\n",
        "\n",
        "    return word_to_idx\n",
        "\n",
        "# generate BoW feature for text input X.\n",
        "def extract_features_BoW(X, word_to_idx):\n",
        "    \"\"\"\n",
        "    Generates BoW feature for X using word_to_idx.\n",
        "\n",
        "    Args:\n",
        "        X (string): text input.\n",
        "        word_to_idx (dictionary): word-index mapping with words as keys and\n",
        "                                  indices as values.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: features of X.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    # ADD YOUR CODE HERE\n",
        "    words = X.split(' ')\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_to_idx:\n",
        "            idx = word_to_idx[word]\n",
        "            features[idx] = features.get(idx, 0) + 1\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HiruQeAvMr0",
        "outputId": "a420cb74-b994-4dd6-f3d4-73e26cbb5742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{2: 3, 3: 2, 1: 1}\n"
          ]
        }
      ],
      "source": [
        "# checking the outputs of the above functions on a small set of examples\n",
        "\n",
        "sample_data = [\n",
        "    \"When is the homework due ?\",\n",
        "    \"When are the TAs' office hours ?\",\n",
        "    \"How hard is the homework ?\",\n",
        "]\n",
        "word_to_idx = word_to_idx_BOW(sample_data)\n",
        "features = extract_features_BoW(sample_data[0], word_to_idx)\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ErmQIuFWTpa_"
      },
      "outputs": [],
      "source": [
        "def get_learnable_weights_BoW(X_data, Y_data, word_to_idx_BOW, extract_features_BoW):\n",
        "    \"\"\"\n",
        "    Learning feature weights for BoW features, using training data.\n",
        "\n",
        "    Args:\n",
        "        X_data (list of strings): All the text data points in training data.\n",
        "        Y_data (list of int): Ground truth labels for text data points in X_data.\n",
        "        word_to_idx_BOW: A Function that looks at all the words in data corpus X_data\n",
        "                         and returns word-index mapping. You had to implement\n",
        "                         word_to_idx_BOW() funtion above.\n",
        "        extract_features_BoW: A Function that extracts BoW features from text sample.\n",
        "                              The extract_features_BoW() function had to be implemented\n",
        "                              by you above.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: feature names and their learned weights.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build a word->count mapping from corpus (word_to_idx_BOW may return counts)\n",
        "    raw_word_counts = word_to_idx_BOW(X_data)\n",
        "\n",
        "    # If the mapper already returns indices (unlikely in this implementation),\n",
        "    # detect and convert counts->indices appropriately. We expect raw_word_counts\n",
        "    # to be a dict word->count; convert to word->idx (0..V-1) sorted by freq.\n",
        "    if raw_word_counts and all(isinstance(v, int) for v in raw_word_counts.values()):\n",
        "        # sort words by frequency (descending)\n",
        "        sorted_words = sorted(raw_word_counts.items(), key=lambda x: -x[1])\n",
        "        word_list = [w for w, _ in sorted_words]\n",
        "    else:\n",
        "        # fallback: use keys as-is\n",
        "        word_list = list(raw_word_counts.keys())\n",
        "\n",
        "    # create final word->idx mapping\n",
        "    word_to_idx = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "    # build feature matrix\n",
        "    num_samples = len(X_data)\n",
        "    vocab_size = len(word_to_idx)\n",
        "\n",
        "    # avoid huge memory by using a dense list of lists; acceptable for assignment sizes\n",
        "    all_features = []\n",
        "    for text in X_data:\n",
        "        feat_vec = [0] * vocab_size\n",
        "        feat_dict = extract_features_BoW(text, word_to_idx)\n",
        "        for idx, cnt in feat_dict.items():\n",
        "            # idx might be int or string; ensure int\n",
        "            try:\n",
        "                i = int(idx)\n",
        "            except Exception:\n",
        "                # if idx is a word, map it\n",
        "                i = word_to_idx.get(idx, None)\n",
        "            if i is None:\n",
        "                continue\n",
        "            if 0 <= i < vocab_size:\n",
        "                feat_vec[i] = cnt\n",
        "        all_features.append(feat_vec)\n",
        "\n",
        "    # train logistic regression on BoW features\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    clf = LogisticRegression(fit_intercept=False, random_state=42, max_iter=1000)\n",
        "    clf.fit(all_features, Y_data)\n",
        "\n",
        "    coeffs = clf.coef_[0]\n",
        "\n",
        "    # map index -> weight\n",
        "    coffs_dict = {i: float(coeffs[i]) for i in range(len(coeffs))}\n",
        "\n",
        "    # Return both the weights and the word_to_idx mapping so predict can use it\n",
        "    return {'weights': coffs_dict, 'word_to_idx': word_to_idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJSKciYNYNXx",
        "outputId": "724b26ba-1b6e-4954-c93b-f2bf2a37fc53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION of BoW classifier is: 84.275\n"
          ]
        }
      ],
      "source": [
        "def predict(X, feature_weights):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "        feature_weights: weightage of different features.\n",
        "\n",
        "    Returns:\n",
        "        int: binary sentiment represented by 0/1.\n",
        "    \"\"\"\n",
        "    # feature_weights is expected to be a dict returned by get_learnable_weights_BoW\n",
        "    if not feature_weights:\n",
        "        return 1\n",
        "\n",
        "    if isinstance(feature_weights, dict) and 'weights' in feature_weights and 'word_to_idx' in feature_weights:\n",
        "        weights = feature_weights['weights']\n",
        "        word_to_idx = feature_weights['word_to_idx']\n",
        "    else:\n",
        "        # backward compatibility: if a raw weights dict keyed by indices was provided\n",
        "        weights = feature_weights\n",
        "        word_to_idx = {}\n",
        "\n",
        "    feats = extract_features_BoW(X, word_to_idx)\n",
        "    score = 0.0\n",
        "    for idx, cnt in feats.items():\n",
        "        try:\n",
        "            i = int(idx)\n",
        "        except Exception:\n",
        "            i = word_to_idx.get(idx, None)\n",
        "        if i is None:\n",
        "            continue\n",
        "        score += cnt * float(weights.get(i, 0.0))\n",
        "\n",
        "    return 1 if score > 0 else 0\n",
        "\n",
        "# get the sample weights\n",
        "BoW_feature_weights_lr = get_learnable_weights_BoW(X_train, y_train, word_to_idx_BOW, extract_features_BoW)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = predict(input_example, BoW_feature_weights_lr)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (f\"EVALUATION of BoW classifier is: {calculate_accuracy(y_test, predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DPrrDVmF4qs"
      },
      "source": [
        "## Finding most positive and most negative words (2 Marks)\n",
        "\n",
        "Based on the magnitude of weights corresponding to different words, write down code to find the 5 most positive words and 5 most negative words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6aX7UrbF_cx",
        "outputId": "6a5e7124-489d-46eb-9a94-31592e478c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EVALUATION five most positive words: wonderfully subtle />7/10 emotions brutal\n",
            "EVALUATION five most negative words: waste poorly worst fails tedious\n"
          ]
        }
      ],
      "source": [
        "most_positive_words = []\n",
        "most_negative_words = []\n",
        "\n",
        "## WRITE CODE HERE TO POPULATE THESE LISTS\n",
        "## most_postive_words and most_negative_words should be list of strings\n",
        "\n",
        "#get feature set\n",
        "BoW_feature_weights_lr = get_learnable_weights_BoW(X_train, y_train, word_to_idx_BOW, extract_features_BoW)\n",
        "\n",
        "# Extract word_to_idx mapping and weights from the learned feature weights\n",
        "word_to_idx = BoW_feature_weights_lr['word_to_idx']\n",
        "weights = BoW_feature_weights_lr['weights']\n",
        "\n",
        "# Create a list of (word, weight) tuples\n",
        "word_weights = []\n",
        "# Iterate through the word_to_idx to get the word string and its corresponding weight\n",
        "# We need to reverse the mapping from index to word for lookup\n",
        "idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
        "\n",
        "for idx, weight in weights.items():\n",
        "    word = idx_to_word.get(idx) # Get the word string using the index\n",
        "    if word:\n",
        "        word_weights.append((word, weight))\n",
        "\n",
        "# Sort by weight in descending order for most positive words\n",
        "word_weights_sorted_desc = sorted(word_weights, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Sort by weight in ascending order for most negative words\n",
        "word_weights_sorted_asc = sorted(word_weights, key=lambda item: item[1])\n",
        "\n",
        "# Get the top 5 most positive words\n",
        "most_positive_words = [word for word, _ in word_weights_sorted_desc[:5]]\n",
        "\n",
        "# Get the top 5 most negative words\n",
        "most_negative_words = [word for word, _ in word_weights_sorted_asc[:5]]\n",
        "\n",
        "\n",
        "\n",
        "assert (len(most_positive_words) == 5)\n",
        "assert (len(most_negative_words) == 5)\n",
        "\n",
        "print(\"EVALUATION five most positive words: \" + \" \".join(most_positive_words))\n",
        "print(\"EVALUATION five most negative words: \" + \" \".join(most_negative_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48UudOSjLnh"
      },
      "source": [
        "## Part II Word2Vec (TA: Nicy Scaria)  (25 Marks)\n",
        "\n",
        "Word2vec is one of the most popular techniques to learn word embeddings. The idea behind word2vec was that the meaning of a word is determined by the context in which it occurs. A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
        "\n",
        "**Word2vec** model has 2 architectures:\n",
        "\n",
        "1. **Continuous bag of word (CBOW):**\n",
        "\n",
        "    CBOW predicts the center word from the surrounding context words.\n",
        "\n",
        "2. **Skip-gram:**\n",
        "\n",
        "    Skip-gram predicts surrounding context words from the center word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baausB5gkhhT"
      },
      "source": [
        "#### SkipGram from Scratch\n",
        "\n",
        "In this exercise, you will code the skipgram model from scratch using the PyTorch library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LqI4ImZcP-I",
        "outputId": "62cb74c3-dda1-4647-a6a4-d4e7b277dda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running: /usr/bin/python3 -m pip uninstall -y torchtext libtorchtext\n",
            "Running: /usr/bin/python3 -m pip install -q --upgrade --no-cache-dir torch\n",
            "Running: /usr/bin/python3 -m pip install -q --upgrade --no-cache-dir portalocker\n",
            "Downloading WikiText-2 zip from: https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Download attempt 1 failed: HTTP Error 301: Moved Permanently. Retrying in 2s...\n",
            "Download attempt 2 failed: HTTP Error 301: Moved Permanently. Retrying in 4s...\n",
            "Download attempt 3 failed: HTTP Error 301: Moved Permanently. Retrying in 8s...\n",
            "Zip download/extract failed: RuntimeError('Failed to download https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip')\n",
            "Falling back to HuggingFace datasets (wikitext-2-raw-v1)...\n",
            "Running: /usr/bin/python3 -m pip install -q --upgrade --no-cache-dir datasets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0521b953ca904d7a8f0217da265d3e8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c98cffe70dc4718b2e98931a22183e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41146322d45e4bccab458951ab2ae47f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd4148ab7e8f4f13941c4103104e0057",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd8ffdf20ddf4210b0f2f3e00d0bc6c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6be2b33ea66f4af1b1d2a604bb2f5846",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "009cffe6c3254485951ba511331ab7e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch = 2.9.1+cu128\n",
            "Setup complete. Next: run the import cell.\n"
          ]
        }
      ],
      "source": [
        "# Colab setup (run once): install torch + get WikiText-2 data (NO torchtext)\n",
        "\n",
        "# Why: torchtext often fails on Colab/Python 3.12 due to binary ABI mismatches (libtorchtext.so errors).\n",
        "# We'll avoid torchtext completely and use the raw WikiText-2 files.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "def _pip(*args):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", *args]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Remove torchtext if present (prevents accidental imports later).\n",
        "_pip(\"uninstall\", \"-y\", \"torchtext\", \"libtorchtext\")\n",
        "\n",
        "# Ensure torch is installed (Colab usually has it, but keep it consistent).\n",
        "_pip(\"install\", \"-q\", \"--upgrade\", \"--no-cache-dir\", \"torch\")\n",
        "\n",
        "# Small dependency used in the assignment\n",
        "_pip(\"install\", \"-q\", \"--upgrade\", \"--no-cache-dir\", \"portalocker\")\n",
        "\n",
        "data_dir = \"./data\"  # keep consistent with later cells\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "extract_dir = os.path.join(data_dir, \"wikitext-2\")\n",
        "train_path = os.path.join(extract_dir, \"wiki.train.tokens\")\n",
        "valid_path = os.path.join(extract_dir, \"wiki.valid.tokens\")\n",
        "test_path  = os.path.join(extract_dir, \"wiki.test.tokens\")\n",
        "\n",
        "def _have_wikitext2_files():\n",
        "    return all(os.path.exists(p) for p in [train_path, valid_path, test_path])\n",
        "\n",
        "def _download_file(url: str, dest: str, retries: int = 3, timeout: int = 30):\n",
        "    last_err = None\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            req = urllib.request.Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "            with urllib.request.urlopen(req, timeout=timeout) as r, open(dest, \"wb\") as f:\n",
        "                f.write(r.read())\n",
        "            return\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            wait = min(10, 2 ** attempt)\n",
        "            print(f\"Download attempt {attempt} failed: {e}. Retrying in {wait}s...\")\n",
        "            time.sleep(wait)\n",
        "    raise RuntimeError(f\"Failed to download {url}\") from last_err\n",
        "\n",
        "def _extract_zip(zip_path: str, out_dir: str):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(out_dir)\n",
        "\n",
        "def _prepare_wikitext2_from_zip():\n",
        "    # Official WikiText-2 zip (sometimes flaky on Colab).\n",
        "    url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\"\n",
        "    zip_path = os.path.join(data_dir, \"wikitext-2-v1.zip\")\n",
        "    print(\"Downloading WikiText-2 zip from:\", url)\n",
        "    _download_file(url, zip_path)\n",
        "    print(\"Extracting...\")\n",
        "    _extract_zip(zip_path, data_dir)\n",
        "    if not _have_wikitext2_files():\n",
        "        raise RuntimeError(\"Zip extracted but expected wiki.*.tokens files were not found\")\n",
        "\n",
        "def _prepare_wikitext2_from_huggingface():\n",
        "    # Robust fallback: fetch WikiText-2 raw via HuggingFace datasets and write the expected files.\n",
        "    print(\"Falling back to HuggingFace datasets (wikitext-2-raw-v1)...\")\n",
        "    _pip(\"install\", \"-q\", \"--upgrade\", \"--no-cache-dir\", \"datasets\")\n",
        "    from datasets import load_dataset  # noqa: E402\n",
        "\n",
        "    ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "    # HF uses 'validation' split name; we write it as wiki.valid.tokens to match the notebook.\n",
        "    split_map = {\n",
        "        \"train\": train_path,\n",
        "        \"validation\": valid_path,\n",
        "        \"test\": test_path,\n",
        "    }\n",
        "    for split_name, out_path in split_map.items():\n",
        "        lines = ds[split_name][\"text\"]\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for line in lines:\n",
        "                f.write(line)\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "if _have_wikitext2_files():\n",
        "    print(\"WikiText-2 already present at\", extract_dir)\n",
        "else:\n",
        "    # Try zip first; if it fails (wget exit 8 / intermittent S3 issues), use HF datasets fallback.\n",
        "    try:\n",
        "        _prepare_wikitext2_from_zip()\n",
        "    except Exception as e:\n",
        "        print(\"Zip download/extract failed:\", repr(e))\n",
        "        _prepare_wikitext2_from_huggingface()\n",
        "\n",
        "assert _have_wikitext2_files(), \"WikiText-2 dataset files are still missing after setup.\"\n",
        "\n",
        "import torch  # noqa: E402\n",
        "print(\"torch =\", torch.__version__)\n",
        "print(\"Setup complete. Next: run the import cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restart runtime (recommended)\n",
        "After the setup cell finishes, do **Runtime → Restart runtime** (Colab).\n",
        "\n",
        "Then run the next cell (imports). Restarting ensures the environment is clean after package installs/downloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "_pTSKA5fiVQs",
        "outputId": "65803580-09b7-4a4f-e8ef-33b712dd151d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch = 2.9.1+cu128\n"
          ]
        }
      ],
      "source": [
        "# importing the necessary libraries (torchtext-free)\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"torch =\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lu8GMxMlZ0f"
      },
      "source": [
        "> **In the following code the add your hyperparameters for the network.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZeXkPW6URMu"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "\n",
        "\n",
        "\"\"\"choose your hyperparameter and see the difference in performance\"\"\"\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "\n",
        "# CHANGE THE None VALUES TO YOUR DESIRED VALUES\n",
        "# Please free to play with these hyperparameters to see the effects on the\n",
        "# quality of generated embeddings\n",
        "\n",
        "\n",
        "SKIPGRAM_N_WORDS = None # the length of the context on each side (k)\n",
        "\n",
        "MIN_WORD_FREQUENCY = None # only words with a minimum word frequency considered\n",
        "MAX_SEQUENCE_LENGTH = None # sentences with length more than this value truncated\n",
        "\n",
        "EMBED_DIMENSION = None # dimension of the word2vec vectors\n",
        "\n",
        "EMBED_MAX_NORM = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxyJ0Q1fT62V"
      },
      "outputs": [],
      "source": [
        "# torchtext-free utilities for WikiText-2 + vocab building\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def get_english_tokenizer():\n",
        "    \"\"\"A basic_english-like tokenizer (pure Python).\"\"\"\n",
        "    # Mimic torchtext basic_english roughly: lowercase and keep simple word tokens + contractions\n",
        "    return lambda s: re.findall(r\"[a-z]+'t|[a-z]+\", s.lower())\n",
        "\n",
        "def _wikitext2_path(data_dir: str, split: str) -> str:\n",
        "    # WikiText-2 file names inside wikitext-2-v1.zip\n",
        "    mapping = {\n",
        "        \"train\": \"wiki.train.tokens\",\n",
        "        \"valid\": \"wiki.valid.tokens\",\n",
        "        \"test\": \"wiki.test.tokens\",\n",
        "    }\n",
        "    if split not in mapping:\n",
        "        raise ValueError(f\"Unknown split={split}. Expected one of {list(mapping.keys())}\")\n",
        "    return os.path.join(data_dir, \"wikitext-2\", mapping[split])\n",
        "\n",
        "def get_data_iterator(ds_name, ds_type, data_dir):\n",
        "    \"\"\"\n",
        "    Returns an iterator over lines of WikiText-2 (pure Python).\n",
        "    ds_name is kept for API compatibility with the notebook; it is ignored.\n",
        "    ds_type should be one of: 'train', 'valid', 'test'.\n",
        "    \"\"\"\n",
        "    path = _wikitext2_path(data_dir, ds_type)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find WikiText-2 split file at {path}. \"\n",
        "            \"Run the setup cell in the Word2Vec section, then restart runtime.\"\n",
        "        )\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                yield line\n",
        "\n",
        "class SimpleVocab:\n",
        "    \"\"\"A minimal torchtext-like vocab wrapper.\"\"\"\n",
        "    def __init__(self, stoi, unk_token=\"<unk>\"):\n",
        "        self._stoi = dict(stoi)\n",
        "        self._itos = {i: w for w, i in self._stoi.items()}\n",
        "        self._unk = unk_token\n",
        "        self._default_index = self._stoi.get(unk_token, 0)\n",
        "    def set_default_index(self, idx):\n",
        "        self._default_index = int(idx)\n",
        "    def __call__(self, tokens):\n",
        "        return [self._stoi.get(t, self._default_index) for t in tokens]\n",
        "    def __getitem__(self, token):\n",
        "        # Behave like torchtext vocab: return index for token (or unk index)\n",
        "        return self._stoi.get(token, self._default_index)\n",
        "    def __contains__(self, token):\n",
        "        return token in self._stoi\n",
        "    def get_stoi(self):\n",
        "        return self._stoi\n",
        "    def lookup_token(self, idx):\n",
        "        return self._itos.get(int(idx), self._unk)\n",
        "    def __len__(self):\n",
        "        return len(self._stoi)\n",
        "\n",
        "def build_vocab(data_iter, tokenizer):\n",
        "    \"\"\"Builds a vocabulary from an iterator of text lines.\"\"\"\n",
        "    counter = Counter()\n",
        "    for line in data_iter:\n",
        "        counter.update(tokenizer(line))\n",
        "    min_freq = MIN_WORD_FREQUENCY if MIN_WORD_FREQUENCY is not None else 1\n",
        "    words = [w for w, c in counter.most_common() if c >= min_freq]\n",
        "    stoi = {\"<unk>\": 0}\n",
        "    for w in words:\n",
        "        if w == \"<unk>\":\n",
        "            continue\n",
        "        stoi[w] = len(stoi)\n",
        "    vocab = SimpleVocab(stoi)\n",
        "    vocab.set_default_index(stoi[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "def to_map_style_dataset(iterable):\n",
        "    \"\"\"Materialize an iterable into a list so DataLoader can index it.\"\"\"\n",
        "    return list(iterable)\n",
        "\n",
        "def collate_skipgram(batch, text_pipeline):\n",
        "    \"\"\"\n",
        "    Prepare (center, context) pairs for skip-gram training.\n",
        "    \"\"\"\n",
        "    batch_input, batch_output = [], []\n",
        "    for text in batch:\n",
        "        text_tokens_ids = text_pipeline(text)\n",
        "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
        "            continue\n",
        "        if MAX_SEQUENCE_LENGTH:\n",
        "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
        "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
        "            window = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
        "            center = window[SKIPGRAM_N_WORDS]\n",
        "            context = window[:SKIPGRAM_N_WORDS] + window[SKIPGRAM_N_WORDS+1:]\n",
        "            for c in context:\n",
        "                batch_input.append(center)\n",
        "                batch_output.append(c)\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    return batch_input, batch_output\n",
        "\n",
        "def get_dataloader_and_vocab(\n",
        "    model_name, ds_name, ds_type, data_dir, batch_size, shuffle, vocab=None\n",
        "    ):\n",
        "    data_iter = get_data_iterator(ds_name, ds_type, data_dir)\n",
        "    tokenizer = get_english_tokenizer()\n",
        "    data_iter = to_map_style_dataset(data_iter)\n",
        "    if vocab is None:\n",
        "        vocab = build_vocab(data_iter, tokenizer)\n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "    dataloader = DataLoader(\n",
        "        data_iter,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate_skipgram, text_pipeline=text_pipeline),\n",
        "    )\n",
        "    return dataloader, vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Ap0SHCloMj"
      },
      "source": [
        "### Initialize the SkipGram Model\n",
        "\n",
        " **Complete the `initialization` and `forward` function in the following  SkipGram_Model class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NNvSmAgYydH"
      },
      "outputs": [],
      "source": [
        "class SkipGram_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Skip-Gram model described in paper:\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(SkipGram_Model, self).__init__()\n",
        "\n",
        "        \"\"\"define the embedding and the linear layer of the network\"\"\"\n",
        "        # this is the initialization for the layers in the skipgram model\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        \"\"\"define forward function\"\"\"\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # return the output of your final layer to find the minimize the loss\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_word_embedding(self):\n",
        "        \"\"\" return the associated word embeddings for center words \"\"\"\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # return the normalized embeddings as a 2D numpy array (in word2vec models,\n",
        "        # the vector associated with the center word is considered\n",
        "        # word embedding)\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcqxAiTSlwbw"
      },
      "source": [
        "> **The following is the Trainer class for the skip-gram model. Add your code for the `training` and `validation` loops.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wSYVSMlZFwA"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Main class for model training\"\"\"\n",
        "\n",
        "    # NOTE: you are free to add additional inputs/functions\n",
        "    # to the trainer class to make training better\n",
        "    # make sure to define and add it within the input\n",
        "    # and initialization if you are using any additional inputs\n",
        "    # for usage in the function\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device,\n",
        "        model_dir,\n",
        "        model_name,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.model_dir = model_dir\n",
        "        self.model_name = model_name\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    # ADD YOUR CODE HERE FOR TRAINING & VALIDATION\n",
        "    # This implementation need not include negative sampling.\n",
        "\n",
        "    # NOTE: you can add additional functions to make training better\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to directory\n",
        "\n",
        "        \"\"\"\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"\n",
        "        Save train/val loss as json file to the directory\n",
        "\n",
        "        \"\"\"\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BgM-DGTl4qy"
      },
      "source": [
        "> **The following code block defines the various parameters and nomenclature for the training and saving of the skip-gram model. Add numerical values for the `specified hyperparameters`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzoqreghaSgc"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "# CHANGE THE None VALUES TO YOUR DESIRED VALUES\n",
        "\n",
        "model_name = 'skipgram'\n",
        "\n",
        "dataset = 'WikiText2'\n",
        "data_dir = './data/'\n",
        "train_batch_size = None\n",
        "val_batch_size = None\n",
        "shuffle = True\n",
        "\n",
        "optimizer = None\n",
        "learning_rate = None\n",
        "epochs = None\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# change the directory name with your SAPname and SRno\n",
        "\n",
        "model_dir = 'SAPname_SRno'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSE1qsvxmEdP"
      },
      "source": [
        "> **The following code block is used to train and save the model. Add the code wherever required**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zMDRVHMZWO6"
      },
      "outputs": [],
      "source": [
        "os.makedirs(model_dir)\n",
        "\n",
        "train_dataloader, vocab = get_dataloader_and_vocab(\n",
        "    model_name=model_name,\n",
        "    ds_name=dataset,\n",
        "    ds_type=\"train\",\n",
        "    data_dir=data_dir,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    vocab=None,\n",
        ")\n",
        "\n",
        "val_dataloader, _ = get_dataloader_and_vocab(\n",
        "    model_name=model_name,\n",
        "    ds_name=dataset,\n",
        "    ds_type=\"valid\",\n",
        "    data_dir=data_dir,\n",
        "    batch_size=val_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    vocab=vocab,\n",
        ")\n",
        "\n",
        "vocab_size = len(vocab.get_stoi())\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "model_class = SkipGram_Model\n",
        "model = model_class(vocab_size=vocab_size)\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# You'll have to specify the loss criterion\n",
        "# Your loss function would depend upon whether you\n",
        "# choose to train with negative sampling or not\n",
        "# either of the two are valid choices\n",
        "\n",
        "criterion = None\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# You'll have to specify the optimizer here\n",
        "optimizer = None\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# NOTE: if you are **optionally** using additional options for the trainer\n",
        "# (e.g., a training scheduler), please add them below.\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    epochs=epochs,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    model_dir=model_dir,\n",
        "    model_name=model_name,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.save_loss()\n",
        "vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "torch.save(vocab, vocab_path)\n",
        "print(\"Model artifacts saved to folder:\", model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1PKIac1mMsH"
      },
      "source": [
        "### Let us analyze the performance of the model\n",
        "\n",
        "You'll be evaluated on the quality of the word representations as judged by the word similarity test, and word analogy tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-K93nQt5CTZ"
      },
      "outputs": [],
      "source": [
        "#@title Evaluation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80d1lRiD7ms4"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "# change the directory name with your SAPname and SRno\n",
        "\n",
        "folder = \"SAPname_SRno\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load the saved model\n",
        "model = torch.load(f\"{folder}/model.pt\", map_location=device)\n",
        "vocab = torch.load(f\"{folder}/vocab.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc9AMn5GVUKZ"
      },
      "outputs": [],
      "source": [
        "word_embeddings = model.get_word_embedding()\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# change the directory name with your SAPname and SRno\n",
        "\n",
        "# Save the embeddings to the folder\n",
        "np.save('SAPname_SRno/word_embeddings.npy', word_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pPrbet--qJ"
      },
      "source": [
        "Once the embeddings are trained, we can use a few words to evaluate some desirable properties of word representations.\n",
        "\n",
        "For instance, whether similar words are indeed similar in the high-dimensional space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7hq6o3z777b"
      },
      "outputs": [],
      "source": [
        "words = ['king', 'queen', 'river', 'water', 'ocean', 'tree', 'plant', 'happy', 'glad', 'mother', 'daughter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5kAEr9rmYym"
      },
      "source": [
        "> **Write a code to find the similarity of the each word in words with eachother**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCXVzRrh8Dvu"
      },
      "outputs": [],
      "source": [
        "def get_word_similarity(words):\n",
        "  \"\"\"\n",
        "  This function takes the words as input and outputs the word vectors\n",
        "  corresponding to the words obtained from your word2vec model and the\n",
        "  similarity of every word with each other.\n",
        "  word2vec is the embedding matrix corresponding to the given words and\n",
        "  this has to be returned as a numpy array to apply PCA on it whereas,\n",
        "  w2v_similarity[i][j] should contain the similarity of word i with word j\n",
        "\n",
        "  \"\"\"\n",
        "  # ADD YOUR CODE HERE\n",
        "\n",
        "  # you'll have to compute the similarity matrix for the words given above\n",
        "\n",
        "  return word2vec, w2v_similarity\n",
        "\n",
        "word2vec, w2v_similarity = get_word_similarity(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrUjGgstmhx8"
      },
      "source": [
        "Let us visualize this similarity matrix. The similarity of each word with other words in words is displayed as a pandas dataframe and as a heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8OVhSUZ8LZT"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(w2v_similarity, columns = words, index = words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AWL0kqAmqsK"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(pd.DataFrame(w2v_similarity, columns = words, index = words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V12MD_EumuA7"
      },
      "source": [
        "The size of the words embedding are reduced to to 2D and displayed as a scatterplot for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF9qSiwNmwzw"
      },
      "outputs": [],
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_w2v = pca.fit_transform(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS9QF8AE8MTG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(reduced_w2v[:,0],reduced_w2v[:,1], s = 12, color = 'red')\n",
        "plt.xlim([-2,2])\n",
        "plt.ylim([-2,2])\n",
        "x, y = reduced_w2v[:,0] , reduced_w2v[:,1]\n",
        "offset = 0.5\n",
        "for i in range(len(x)):\n",
        "    label = words[i]\n",
        "    xi, yi = x[i], y[i]\n",
        "    plt.annotate(label, (xi, yi), xytext=(xi + offset, yi + offset),\n",
        "                 textcoords='offset points', ha='center', va='center')\n",
        "\n",
        "plt.savefig(\"word_similarity.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4w3UUE1m4W_"
      },
      "source": [
        "The 10 most similar word to the given word is calculated in the following code blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvXS4uzd8O7W"
      },
      "outputs": [],
      "source": [
        "def get_top_similar(word: str, topN: int = 10):\n",
        "\n",
        "    \"\"\"\n",
        "    This function calculates the topN words similar to the input word.\n",
        "    If the word is not in vocabulary, then similarity is not calculated.\n",
        "    If the word is in the vocabulary, then the dot product of the embedding\n",
        "    matrix and the word vector is calculated. The topN words are selected.\n",
        "    \"\"\"\n",
        "    word_id = vocab[word]\n",
        "    if word_id == 0:\n",
        "        print(\"Out of vocabulary word\")\n",
        "        return\n",
        "\n",
        "    word_vec = model.get_word_embedding[word_id]\n",
        "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
        "\n",
        "    dists = np.matmul(model.get_word_embedding, word_vec).flatten()\n",
        "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
        "\n",
        "    topN_dict = {}\n",
        "    for sim_word_id in topN_ids:\n",
        "        sim_word = vocab.lookup_token(sim_word_id)\n",
        "        topN_dict[sim_word] = dists[sim_word_id]\n",
        "\n",
        "    return topN_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atY4cy0J8Tfi"
      },
      "outputs": [],
      "source": [
        "for word, sim in get_top_similar(\"india\").items():\n",
        "   print(\"EVALUATION: most similar words to {}: {:.3f}\".format(word, sim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz-elU-5nAL1"
      },
      "source": [
        "### Analogy Tests\n",
        "\n",
        "Analogy tests include questions of the format a:b::x:?, such tests are used to intrinsically evaluate the quality of word vectors.\n",
        "\n",
        "Here's one example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KrTVR2BnC9N"
      },
      "outputs": [],
      "source": [
        "def get_analogy(word_1, word_2, word_3):\n",
        "\n",
        "  \"\"\"\n",
        "  top 5 most analgous vector calculated correspond to a set similar to\n",
        "  man: woman :: king: ? . This is calculated similar to the above case.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  emb1 = model.get_word_embedding[vocab[word_1]]\n",
        "  emb2 = model.get_word_embedding[vocab[word_2]]\n",
        "  emb3 = model.get_word_embedding[vocab[word_3]]\n",
        "\n",
        "  emb4 = emb1 - emb2 + emb3\n",
        "\n",
        "  # compute dot products between 'emb4' and all word embeddings in the model.\n",
        "  emb4 = np.reshape(emb4, (len(emb4), 1))\n",
        "  dot_product = np.matmul(model.get_word_embedding, emb4).flatten()\n",
        "\n",
        "  top5 = np.argsort(-dot_product)[:5]\n",
        "\n",
        "  return top5, dot_product\n",
        "\n",
        "top5_analogy, dot_product = get_analogy('king', 'man', 'woman')\n",
        "\n",
        "for word_id in top5_analogy:\n",
        "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dot_product[word_id]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKTVUVPEnGVN"
      },
      "source": [
        "The model performance will be evaluated based on an analogy output for the top 5 words. The following code will used to evaluate the performance of the model on the analogies dataset. We will measure how often the correct answer is a part of the top 5 options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2hbpb_h5KgN",
        "outputId": "51993f16-153e-48e9-b218-6c399b32db41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-22 11:22:20--  https://drive.google.com/uc?export=download&id=1jHx25dECegjtRKBB587nEfHiJesrH0g2\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.31.101, 74.125.31.102, 74.125.31.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.31.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1jHx25dECegjtRKBB587nEfHiJesrH0g2&export=download [following]\n",
            "--2024-01-22 11:22:20--  https://drive.usercontent.google.com/download?id=1jHx25dECegjtRKBB587nEfHiJesrH0g2&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 286075 (279K) [application/octet-stream]\n",
            "Saving to: ‘analogies.txt’\n",
            "\n",
            "analogies.txt       100%[===================>] 279.37K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-01-22 11:22:20 (89.7 MB/s) - ‘analogies.txt’ saved [286075/286075]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading the file containing a few analogies.\n",
        "# We will change the contents of this file while testing.\n",
        "\n",
        "!wget -O analogies.txt \"https://drive.google.com/uc?export=download&id=1jHx25dECegjtRKBB587nEfHiJesrH0g2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obbMdoMe1Tw7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def load_and_sample_analogies(file_path, sample_size=5000):\n",
        "    with open(file_path, 'r') as file:\n",
        "        analogies = []\n",
        "        for line in file:\n",
        "            # Split the line into words and ensure it has exactly 4 elements\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 4:\n",
        "                analogies.append(parts)\n",
        "\n",
        "        # Sample analogies\n",
        "        sampled_analogies = random.sample(analogies, min(sample_size, len(analogies)))\n",
        "        return sampled_analogies\n",
        "\n",
        "# Path to your text file\n",
        "# NOTE: analogies used for grading could be slightly different\n",
        "file_path = '/content/analogies.txt'\n",
        "\n",
        "# Load and sample analogies\n",
        "sampled_analogy_dataset = load_and_sample_analogies(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuLvO2cV1W2j"
      },
      "outputs": [],
      "source": [
        "def get_word_id(word, vocab):\n",
        "    \"\"\"check for out of vocabulary items\"\"\"\n",
        "    return vocab[word] if word in vocab else 0\n",
        "\n",
        "def analogy_score(analogy_dataset):\n",
        "    \"\"\"\n",
        "    The top5 analogous words calculated for each set of words for your\n",
        "    implementation of word2vec and compared with an existing dataset to\n",
        "    calculate if the expected word is in the first 5 predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for a, b, c, d in analogy_dataset:\n",
        "        # Convert words to lowercase\n",
        "        a, b, c, d = a.lower(), b.lower(), c.lower(), d.lower()\n",
        "\n",
        "        # Check if any word is out of vocabulary\n",
        "        if 0 in [get_word_id(word, vocab) for word in [a, b, c, d]]:\n",
        "            continue\n",
        "\n",
        "        #finding the first five words that are analogous to the given set\n",
        "        top5_analogy, dot_product = get_analogy(a, b, c)\n",
        "\n",
        "        predicted_words = []\n",
        "\n",
        "        for word_id in top5_analogy:\n",
        "            word = vocab.lookup_token(word_id)\n",
        "            predicted_words.append(word)\n",
        "\n",
        "        if d in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    precision_at_5 = correct / total if total > 0 else 0\n",
        "    return precision_at_5\n",
        "\n",
        "precision_at_5 = analogy_score(sampled_analogy_dataset)\n",
        "print(\"EVALUATION: Precision at 5 for the analogy test is\", precision_at_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwjiemeOnKvj"
      },
      "source": [
        "#### Google's word2vec for comparison\n",
        "\n",
        "In the following code blocks, the pretained word2vec developed by Google is used to analyze the quality of the embedding. The word2vec model can be downloaded from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KhiakvJ3wwe",
        "outputId": "b833be2d-b1b8-4417-c575-454b40191526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-22 11:52:16--  https://drive.google.com/uc?export=download&id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.31.100, 74.125.31.102, 74.125.31.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.31.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg&export=download [following]\n",
            "--2024-01-22 11:52:16--  https://drive.usercontent.google.com/download?id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446 (2.4K) [text/html]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin’\n",
            "\n",
            "\r          GoogleNew   0%[                    ]       0  --.-KB/s               \rGoogleNews-vectors- 100%[===================>]   2.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-22 11:52:16 (17.9 MB/s) - ‘GoogleNews-vectors-negative300.bin’ saved [2446/2446]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O GoogleNews-vectors-negative300.bin \"https://drive.google.com/uc?export=download&id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c8bca5cf80de4948ad86ed9b08469b75",
            "3d0983ae13a34bd9a1635309867cb9cd",
            "c8fbb3415ad941d0b138bc5743f77ac3",
            "b2eadb5f487e45f983771b9c4601ee78",
            "89b7ea42b66e4ea5952f13cea491b9bf",
            "c3053e733101432985d8a9e480d2a29b",
            "de9d707864cd47c28ce47383890c023b",
            "499b50c69b2a43e18aacf4214ecd5b7c",
            "27d2150a6c6a4291bcfee282f491df88",
            "b81d3faa449543a183922d2eafefe47e",
            "93633618c22a48b88dda45059886daf4"
          ]
        },
        "id": "KNMjvv4K2xd0",
        "outputId": "3b03f980-b74e-4a11-cd70-7d62f60007e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8bca5cf80de4948ad86ed9b08469b75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/3.39G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded to /content/GoogleNews-vectors-negative300.bin\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "import requests\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "def download_from_drive(drive_link, target_path):\n",
        "    response = requests.get(drive_link, stream=True)\n",
        "    response.raise_for_status()\n",
        "    if 'html' in response.headers['Content-Type']:\n",
        "        response = requests.get(drive_link)\n",
        "        response.raise_for_status()\n",
        "        page = bs4.BeautifulSoup(response.text, features=\"lxml\")\n",
        "        if form := page.find('form', id='download-form'):\n",
        "            id   = form.select_one(\"input[name='id']\")['value']\n",
        "            uuid = form.select_one(\"input[name='uuid']\")['value']\n",
        "            data = { 'confirm': 't', 'export': 'download', 'id': id, 'uuid': uuid }\n",
        "            response = requests.get(page.find('form')['action'], params=data, stream=True)\n",
        "            response.raise_for_status()\n",
        "    with open(target_path, 'wb+') as file:\n",
        "        with tqdm.tqdm(\n",
        "            total=int(response.headers['Content-Length']),\n",
        "            unit='B', unit_scale=True, unit_divisor=1024\n",
        "        ) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=4096):\n",
        "                file.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "    print(\"Downloaded to\", target_path)\n",
        "\n",
        "drive_link = \"https://drive.google.com/uc?export=download&id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg\"\n",
        "target_path = \"/content/GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "download_from_drive(drive_link, target_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzoahZOUnQIO"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "# Load Google news 300 vectors file\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep71XY238WpV"
      },
      "outputs": [],
      "source": [
        "# List of words to plot the embeddings\n",
        "words = ['king', 'queen', 'river', 'water', 'ocean', 'tree', 'plant', 'happy', 'glad', 'mother', 'daughter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUtqxPrnWV9"
      },
      "source": [
        "> **Write a code to find the similarity of the each word in words with eachother using original word2vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QLcfJvvnYl9"
      },
      "outputs": [],
      "source": [
        "def get_word_similarity(words):\n",
        "  \"\"\"\n",
        "  This function takes the words as input and outputs the word vectors\n",
        "  corresponding to the words using Google's word2vec and the similarity of\n",
        "  every word with eachother. word2vec is the embedding matrix for the words\n",
        "  given above w2v_similarity[i][j] should contain the similarity of word i with j\n",
        "\n",
        "  \"\"\"\n",
        "  # ADD YOUR CODE HERE\n",
        "\n",
        "  # you'll have to compute the similarity matrix for the words given above\n",
        "\n",
        "  return word2vec, w2v_similarity\n",
        "\n",
        "word2vec, w2v_similarity = get_word_similarity(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLQn4B_snbfg"
      },
      "source": [
        "The similarity of each word with other words in words is displayed as a pandas dataframe and as a heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3-NqzVunhg1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(w2v_similarity, columns = words, index = words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koErrEInnjcd"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(pd.DataFrame(w2v_similarity, columns = words, index = words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptKTusGJnpLy"
      },
      "source": [
        "The size of the words embedding are reduced to to 2D and displayed as a scatterplot for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADEMWok8nrX0"
      },
      "outputs": [],
      "source": [
        "#PCA on word2vec embedding\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "reduced_w2v = pca.fit_transform(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la2osFTxntIc"
      },
      "outputs": [],
      "source": [
        "#plotting reduced order embeddings in a 2-D space\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(reduced_w2v[:,0],reduced_w2v[:,1], s = 12, color = 'red')\n",
        "plt.xlim([-2.5,2.5])\n",
        "plt.ylim([-2.5,2.5])\n",
        "x, y = reduced_w2v[:,0] , reduced_w2v[:,1]\n",
        "for i in range(len(x)):\n",
        "    plt.annotate(words[i],xy=(x[i], y[i]),xytext=(x[i]+0.05,y[i]+0.05))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whg_kv2QnvXW"
      },
      "outputs": [],
      "source": [
        "model.most_similar('india')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIx8JEMynzK5"
      },
      "source": [
        "Analogy test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N41c0AJKnzou"
      },
      "outputs": [],
      "source": [
        "def analogy(x1, x2, y1): #defining analogy function\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1], topn = 5)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbEfS_7An3RF"
      },
      "outputs": [],
      "source": [
        "analogy('man', 'king', 'woman')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbqo3EhNy952"
      },
      "outputs": [],
      "source": [
        "def analogy_score(analogy_dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    The top5 analogous words calculated for each set of words for Google's\n",
        "    word2vec and compared with an existing dataset to calculate if the word\n",
        "    is in the first 5 predictions.\n",
        "\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for a, b, c, d in analogy_dataset:\n",
        "        # Convert words to lowercase\n",
        "        a, b, c, d = a.lower(), b.lower(), c.lower(), d.lower()\n",
        "\n",
        "        words_scores = analogy(a,b,c)\n",
        "\n",
        "        predicted_words = [item[0] for item in words_scores]\n",
        "\n",
        "        if d in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    precision_at_5 = correct / total if total > 0 else 0\n",
        "    return precision_at_5\n",
        "\n",
        "precision_at_5_Google = analogy_score(sampled_analogy_dataset)\n",
        "print(\"EVALUATION: Precision at 5 for the analogy test with Google skip-gram model is\", precision_at_5_Google)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acp_DOcZn5j2"
      },
      "source": [
        "### Submission Instructions\n",
        "\n",
        "Mentioned at the top of the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27d2150a6c6a4291bcfee282f491df88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d0983ae13a34bd9a1635309867cb9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3053e733101432985d8a9e480d2a29b",
            "placeholder": "​",
            "style": "IPY_MODEL_de9d707864cd47c28ce47383890c023b",
            "value": "100%"
          }
        },
        "499b50c69b2a43e18aacf4214ecd5b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b7ea42b66e4ea5952f13cea491b9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93633618c22a48b88dda45059886daf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2eadb5f487e45f983771b9c4601ee78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b81d3faa449543a183922d2eafefe47e",
            "placeholder": "​",
            "style": "IPY_MODEL_93633618c22a48b88dda45059886daf4",
            "value": " 3.39G/3.39G [01:08&lt;00:00, 56.6MB/s]"
          }
        },
        "b81d3faa449543a183922d2eafefe47e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3053e733101432985d8a9e480d2a29b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8bca5cf80de4948ad86ed9b08469b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d0983ae13a34bd9a1635309867cb9cd",
              "IPY_MODEL_c8fbb3415ad941d0b138bc5743f77ac3",
              "IPY_MODEL_b2eadb5f487e45f983771b9c4601ee78"
            ],
            "layout": "IPY_MODEL_89b7ea42b66e4ea5952f13cea491b9bf"
          }
        },
        "c8fbb3415ad941d0b138bc5743f77ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499b50c69b2a43e18aacf4214ecd5b7c",
            "max": 3644258522,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27d2150a6c6a4291bcfee282f491df88",
            "value": 3644258522
          }
        },
        "de9d707864cd47c28ce47383890c023b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
